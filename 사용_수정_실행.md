# 실행방법

- [x] node는 22여야 함 : nvm use 22
- [x] python은 3.11이어야 함(backend) : cd ~/pythonenv3.11/; source bin/activate

backend : cd backend, sh dev.sh
frontend : cd root_of_project, npm run dev
접속 : http://localhost:5173

## ollama server 설정(환경변수)

OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_API_BASE_URL=http://localhost:11434/api

## model parameters

num_ctx : context length (tokens)
num_pred : prediction, summary(response) (tokens)
num_ctx > input length + num_pred 이어야 truncate가 없어짐
그렇다고 무조건 context length를 넉넉히 잡는다고 성능/품질이 좋아지지 않음.
