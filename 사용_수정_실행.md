# 실행방법

- [x] node는 22여야 함 : nvm use 22
- [x] python은 3.11이어야 함(backend) : cd ~/pythonenv3.11/; source bin/activate

backend : cd backend, sh dev.sh
frontend : cd root_of_project, npm run dev
접속 : http://localhost:5173

## ollama server 설정(환경변수)

OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_API_BASE_URL=http://localhost:11434/api

## model parameters

num_ctx : context length (tokens)
num_pred : prediction, summary(response) (tokens)
num_ctx > input length + num_pred 이어야 truncate가 없어짐
그렇다고 무조건 context length를 넉넉히 잡는다고 성능/품질이 좋아지지 않음.

## 소스 수정 내역

fix: CORS configuration for local development

- Update backend/dev.sh to use CORS_ORIGINS environment variable
- Modify CORS middleware configuration in main.py to allow localhost:5173
- Replace CORS_ALLOW_ORIGIN with explicit localhost URL for development

# 원격 브라우저에서 실행하기

1. Chrome을 원격 디버깅 모드로 실행합니다:

   macOS (사용자 데이터 디렉토리 지정):

   ```bash
   /Applications/Google\ Chrome.app/Contents/MacOS/Google\ Chrome --remote-debugging-port=9222 --no-sandbox --disable-web-security --user-data-dir=/tmp/chrome-debug
   ```
