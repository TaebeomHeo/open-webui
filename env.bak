# Ollama URL for the backend to connect
# The path '/ollama' will be redirected to the specified backend URL
# CAUTION: 아래 2개는 쌍으로 다녀야 함
OLLAMA_BASE_URL='http://localhost:11434'
OLLAMA_API_BASE_URL='http://localhost:11434/api'
# OLLAMA_BASE_URL='http://wwserver:11434'
# OLLAMA_BASE_URL='http://localhost:1234/v1' # FIXME: TODO: CAUTION: not working.. LM studio를 사용하려면 좀 더 공부를 DONE: API KEY = 'lm-studio'로 해야 함

#
WEBUI_NAME='와와 AI'
#
DEFAULT_USER_ROLE=user # 'user', 'pending', 'admin'

# AUTOMATIC1111_BASE_URL="http://localhost:7860"

# DO NOT TRACK
SCARF_NO_ANALYTICS=true
DO_NOT_TRACK=true
ANONYMIZED_TELEMETRY=false

# Use locally bundled version of the LiteLLM cost map json
# to avoid repetitive startup connections
ENABLE_LITELLM="False"
LITELLM_LOCAL_MODEL_COST_MAP="True"

# loglevel : CRITICAL, ERROR, WARNING, INFO, DEBUG
GLOBAL_LOG_LEVEL=DEBUG

# RAG관련
RAG_TOP_K="3"  # default=5
RAG_RELEVANCE_THRESHOLD="0.1" # default=0.0
RAG_EMBEDDING_MODEL="nomic-embed-text:latest"
RAG_EMBEDDING_ENGINE="ollama"

